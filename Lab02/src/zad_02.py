# ÅšciÄ…gnij czystÄ…, bezbÅ‚Ä™dnÄ… bazÄ™ danych z irysami:â€¢ze strony przedmiotuâ€¢lub z Internetu (Google: iris dataset csv)â€¢lub pobierajÄ…c z paczki sklearn datasets. NastÄ™pnie wykorzystujÄ…c technikÄ™ PCA, chcemy skompresowaÄ‡ bazÄ™ danych z czterech do trzech lub dwÃ³ch kolumn numerycznych, nie tracÄ…c przy tym zbyt wiele informacji.

# Wybierz paczkÄ™ i samouczek do zadania, np.:â€¢https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.htmlâ€¢https://notebook.community/hershaw/data-science-101/course/class1/pca/iris/PCA%20-%20Iris%20datasetâ€¢https://builtin.com/machine-learning/pca-in-pythonMoÅ¼esz teÅ¼ skorzystaÄ‡ z programu pokazanego na wykÅ‚adzie:

# Dokonaj PCA na bazie danych. Przyjrzyj siÄ™ nowym kolumnom i wariancjom. Ile kolumn moÅ¼na usunÄ…Ä‡, tak aby zachowaÄ‡ minimum 95% wariancji (strata informacji nie moÅ¼e byÄ‡ wiÄ™ksza niÅ¼ 5%)? KorzystajÄ…c z poniÅ¼szego wzoru, swojÄ… odpowiedÅº uzasadnij.

# ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘ ğ‘–ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘ğ‘—ğ‘– ğ‘ ğ‘ğ‘œğ‘¤ğ‘œğ‘‘ğ‘œğ‘¤ğ‘ğ‘›ğ‘ ğ‘¢ğ‘ ğ‘¢ğ‘›ğ‘–Ä™ğ‘ğ‘–ğ‘’ğ‘š ğ‘– ğ‘œğ‘ ğ‘¡ğ‘ğ‘¡ğ‘›ğ‘–ğ‘â„ ğ‘˜ğ‘œğ‘™ğ‘¢ğ‘šğ‘› = âˆ‘ğ‘‰ğ‘ğ‘Ÿ(ğ‘˜ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘[ğ‘˜])ğ‘›âˆ’1ğ‘˜=ğ‘›âˆ’ğ‘–âˆ‘ğ‘‰ğ‘ğ‘Ÿ(ğ‘˜ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘[ğ‘˜])ğ‘›âˆ’1ğ‘˜=0BazÄ™ danych z usuniÄ™tymi kolumnami zobrazuj na wykresie punktowym, gdzie kaÅ¼dy punkt to irys. JeÅ›li w bazie zostawisz 2 kolumny, to wykres bÄ™dzie na pÅ‚aszczyÅºnie, a jeÅ›li 3, to bÄ™dzie trÃ³jwymiarowy. PrzykÅ‚ady:

# entropia = porozrzucane dane

from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='FlowerType')
print(X.head())

pca_iris = PCA(n_components=2).fit_transform(iris.data)
# print(pca_iris)
# print(pca_iris.explained_variance_ratio_.sum(), "wspÃ³Å‚czynnik wariancji")
# print(pca_iris.components_, "komponenty")
# print(pca_iris.transform(iris.data), "dane irysÃ³w")

# MogÄ™ usunÄ…Ä‡ dwie kolumny, tak aby zachowaÄ‡ minimum 95% wariancji, poniewaÅ¼ suma wspÃ³Å‚czynnikÃ³w wariancji dwÃ³ch kolumn wynosi ponad 97%.

# plt.figure(2, figsize=(8, 6))
# plt.clf()

# Plot the training points
plt.scatter(pca_iris[:, 0], pca_iris[:, 1], s=35, c=y, cmap=plt.cm.brg)
plt.xlabel("P.C. 1")
plt.ylabel("P.C. 2")
# Tworzenie legendy dla klas
legend_labels = iris.target_names
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=plt.cm.brg(i / len(legend_labels))) for i, label in enumerate(legend_labels)]
plt.legend(handles=legend_elements, loc='lower right')
plt.show()

# Dodanie legendy do wykresu
